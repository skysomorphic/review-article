
@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	pages = {303--314},
	number = {4},
	journaltitle = {Mathematics of Control, Signals and Systems},
	shortjournal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	date = {1989-12-01},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	pages = {359--366},
	number = {5},
	journaltitle = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	date = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
}

@article{pinkus_approximation_1999,
	title = {Approximation theory of the {MLP} model in neural networks},
	volume = {8},
	doi = {10.1017/S0962492900002919},
	pages = {143--195},
	journaltitle = {Acta Numerica},
	author = {Pinkus, Allan},
	date = {1999},
}

@inproceedings{kidger_universal_2020,
	title = {Universal Approximation with Deep Narrow Networks},
	volume = {125},
	url = {https://proceedings.mlr.press/v125/kidger20a.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {The classical Universal Approximation Theorem holds for neural networks of arbitrary width and bounded depth. Here we consider the natural ‘dual’ scenario for networks of bounded width and arbitrary depth. Precisely, let n be the number of inputs neurons, m be the number of output neurons, and let ρ be any nonaffine continuous function, with a continuous nonzero derivative at some point. Then we show that the class of neural networks of arbitrary depth, width n + m + 2, and activation function ρ, is dense in C(K; {\textbackslash}{mathbbR}{\textasciicircum}m) for K {\textbackslash}subseteq {\textbackslash}{mathbbR}{\textasciicircum}n with K compact. This covers every activation function possible to use in practice, and also includes polynomial activation functions, which is unlike the classical version of the theorem, and provides a qualitative difference between deep narrow networks and shallow wide networks. We then consider several extensions of this result. In particular we consider nowhere differentiable activation functions, density in noncompact domains with respect to the L{\textasciicircum}p-norm, and how the width may be reduced to just n + m + 1 for ‘most’ activation functions.},
	pages = {2306--2327},
	booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
	publisher = {{PMLR}},
	author = {Kidger, Patrick and Lyons, Terry},
	editor = {Abernethy, Jacob and Agarwal, Shivani},
	date = {2020-07-09},
}

@misc{commons_filecolored_2025,
	title = {File:Colored neural network.svg — Wikimedia Commons, the free media repository},
	url = {https://commons.wikimedia.org/w/index.php?title=File:Colored_neural_network.svg&oldid=995727191},
	author = {Commons, Wikimedia},
	date = {2025},
}

@unpublished{lee_introduction_2021,
	title = {Introduction of Machine / Deep Learning},
	url = {https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/regression%20(v16).pdf},
	author = {Lee, Hung-Yi},
	date = {2021},
}

@unpublished{hutter_legged_2022,
	location = {2022 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	title = {Legged Robots on the way from subterranean},
	url = {https://www.youtube.com/watch?v=XwheB2_dyMQ},
	author = {Hutter, Marco},
	date = {2022-05-24},
}
